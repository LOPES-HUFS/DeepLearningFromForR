# μμ „νλ¥Ό μ΄μ©ν• λ¨λΈ ν•™μµ λ°©λ²•

μμ „νλ” μ›λ¦¬λ§ μ΄ν•΄ν•λ©΄ λ©λ‹λ‹¤. κµ³μ΄ μμ „νλ¥Ό μ‚¬μ©ν•μ—¬ λ°μ΄ν„°λ¥Ό ν•™μµν•λ” κ²ƒμ€ μ–΄μ©λ©΄ λ¬΄λ¨ν• μ§“μ…λ‹λ‹¤. μ—­μ „νλ²•μ„ μ‚¬μ©ν•λ©΄ 3λ¶„μ΄λ©΄ λ  κ²ƒμ„ 8μ‹κ°„μ— κ±Έμ³μ„ ν•™μµν•κΈ° λ•λ¬Έμ…λ‹λ‹¤.π‚

κ·ΈλΌμ—λ„ λ¶κµ¬ν•κ³  κµ¬ν„ν•΄ λ³΄μ•μµλ‹λ‹¤. μμ „νλ΅ ν•™μµν•κΈ°...!

λ§μ°¬κ°€μ§€λ΅ μ†κΈ€μ”¨λ΅ μ“΄ μ΄λ―Έμ§€λ¥Ό νλ³„ν•λ” λ”¥λ¬λ‹ κµ¬ν„ κ³Όμ •μ„ μ‚΄ν΄λ³΄λ©΄μ„, μμ „νμ μ›λ¦¬μ™€ ν•¨κ» μμ „νκ°€ λλ¦° μ΄μ λ¥Ό ν’€μ–΄ μ„¤λ…ν•΄ λ³΄λ„λ΅ ν•κ² μµλ‹λ‹¤.

## ν•™μµν•κΈ°

μ°μ„  ν•™μµμ— ν•„μ”ν• MNIST λ°μ΄ν„°λ¥Ό λ¶λ¬μ¤λ”λ° ν•„μ”ν• ν¨ν‚¤μ§€λ¥Ό μ„¤μΉν•©λ‹λ‹¤.

```R
install.packages("dslabs")
library(dslabs)
```

λ‹¤μμΌλ΅ ν•™μµμ— ν•„μ”ν• ν•¨μλ¥Ό λ¶λ¬μµλ‹λ‹¤.

```R
source("./functions.R")
source("./utils.R")
source("./numerical_gradient.R")
source("./TwoLayerNet_model.forward.R")
```

ν•™μµν•  λ„¤νΈμ›ν¬λ¥Ό λ§λ“­λ‹λ‹¤. 

```R
TwoLayerNet <- function(input_size, hidden_size, output_size, weight_init_std = 0.01) {
  W1 <<- weight_init_std*matrix(rnorm(n = input_size*hidden_size), nrow = input_size, ncol = hidden_size)
  b1 <<- matrix(rep(0, hidden_size), nrow=1,ncol=hidden_size)
  W2 <<- weight_init_std*matrix(rnorm(n = hidden_size*output_size), nrow = hidden_size, ncol = output_size)
  b2 <<- matrix(rep(0, output_size), nrow=1,ncol=output_size)
  return(list(input_size, hidden_size, output_size, weight_init_std))
}

TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)
```
κ° νλΌλ―Έν„°μ μλ―Έλ” μ•„λμ™€ κ°™μµλ‹λ‹¤.
 * input_size : μ…λ ¥ λ…Έλ“μ κ°μλ΅, μ—¬κΈ°μ„λ” ν• μ΄λ―Έμ§€μ ν¬κΈ°(28*28)λ¥Ό μλ―Έν•©λ‹λ‹¤. 
 * hidden_size : μ€λ‹‰μΈµμ λ…Έλ“ κ°μλ΅, μ—¬κΈ°μ„λ” 50κ°λ΅ μ„¤μ •ν•μ€μµλ‹λ‹¤.
 * output_size : μ¶λ ¥ λ…Έλ“μ κ°μλ΅, μ«μ 0~9μ κ°’μ„ λ¶„λ¥ν•κΈ° λ•λ¬Έμ— 10μ΄ λ©λ‹λ‹¤.
 * weight_init_std : κ°€μ¤‘μΉ μ΄κΈ°κ°’μ΄ ν° κ°’μ΄ λλ” κ²ƒμ„ λ°©μ§€ν•λ” νλΌλ―Έν„°μ…λ‹λ‹¤.

μ΄μ , μ†κΈ€μ”¨ μ΄λ―Έμ§€λ¥Ό λ¶λ¬μ¤κ³  ν•™μµμ„ μ„ν•΄ ν›λ ¨ μ…‹κ³Ό ν…μ¤νΈ μ…‹μΌλ΅ λ‚λ•λ‹λ‹¤.
```R
mnist_data <- get_data()

x_train_normalize <- mnist_data$x_train
x_test_normalize <- mnist_data$x_test

t_train_onehotlabel <- making_one_hot_label(mnist_data$t_train, 60000,10)
t_test_onehotlabel <- making_one_hot_label(mnist_data$t_test, 10000,10)
```

ν•™μµμ— ν•„μ”ν• νλΌλ―Έν„°λ¥Ό μ„¤μ •ν•©λ‹λ‹¤.

```R
learning_rate <- 0.1
iters_num <- 100
train_loss_list <- data.frame(lossvalue=rep(0, iters_num))
train_size <- dim(x_train_normalize)[1]
batch_size <- 100
```
 κ° λ³€μμ μλ―Έλ” λ‹¤μκ³Ό κ°™μµλ‹λ‹¤.

 * learning_rate : ν•™μµλ¥ λ΅, ν•™μµλ¥ μ΄ λ†’μ„μλ΅ ν•™μµμ΄ λΉ¨λ¦¬ μ§„ν–‰λλ” λ€μ‹ μ— λ μ§„ν–‰λ  μ μμµλ‹λ‹¤.
 * iters_num : ν•™μµ λ°λ³µ νμ
 * train_loss_list : μ†μ‹¤ ν•¨μ κ°’ κΈ°λ΅ λ¦¬μ¤νΈ
 * train_size : μ „μ²΄ ν›λ ¨ μ…‹ κ°μ
 * batch_size : ν›λ ¨ μ…‹μ—μ„ λ½‘μ„ μ΄λ―Έμ§€ κ°μ

μ‚¬μ „ μ¤€λΉ„λ” λ‹¤ λλ‚¬μµλ‹λ‹¤. μ‹¤μ λ΅ μμ „νλ¥Ό μ΄μ©ν•΄μ„ ν•™μµν•΄ λ³΄κ² μµλ‹λ‹¤.

```R
for(i in 1:iters_num){
  batch_mask <- sample(train_size,batch_size)
  x <- x_train_normalize[batch_mask,]
  t <- t_train_onehotlabel[batch_mask,]
  grads <- numerical_gradient(loss, x, t)
  W1 <- W1 - (grads$W1 * learning_rate)
  W2 <- W2 - (grads$W2 * learning_rate)
  b1 <- b1 - (grads$b1 * learning_rate)
  b2 <- b2 - (grads$b2 * learning_rate)
  loss_value <- loss(x, t)
  train_loss_list[i,1] <- loss_value
}
```

λ¨λΈμ„ 100λ² ν•™μµν•λ”λ° κ±Έλ¦¬λ” μ‹κ°„μ€ μµμ† 3μ‹κ°„ μ΄μƒμ΄ μ†μ”κ°€ λ©λ‹λ‹¤. μ°Έκ³ λ΅ [νμ΄μ¬ μ½”λ“](https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/ch04/two_layer_net.py)λ¥Ό λ°λ³µνμ 100λ²μΌλ΅ μ„¤μ •ν•μ—¬ μ‹¤ν–‰μ‹ν‚¤λ©΄ μ•½ 5300μ΄κ°€ κ±Έλ¦½λ‹λ‹¤.

## λ¨λΈν‰κ°€

μ΄μ  μ†μ‹¤ ν•¨μ κ°’κ³Ό λ¨λΈμ μ •ν™•λ„λ¥Ό ν™•μΈν•΄ λ΄…μ‹λ‹¤.
```R
train_loss_list

model.evaluate.forward(x_test_normalize,t_test_onehotlabel)
```
